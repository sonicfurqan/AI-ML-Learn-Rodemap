{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4d4cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import CrossEncoder\n",
    "from sentence_transformers import SparseEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8640e133",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "simple converstion of text to number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fac816d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"paraphrase-MiniLM-L3-v2\")\n",
    "\n",
    "sentences=[\n",
    "    \"This is python script\",\n",
    "    \"This is embeding transformer\",\n",
    "    \"This converts text to numbers\",\n",
    "    \"Clouds are white today outside\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86c7c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# covert sentences to embeddings(numbers)\n",
    "embedings=model.encode(sentences)\n",
    "\n",
    "\n",
    "print('total lines',embedings.shape[0])\n",
    "print('total numbers',embedings.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df64192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Calculate the embedding similarities\n",
    "similarities = model.similarity(embedings, embedings)\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fa49a9",
   "metadata": {},
   "source": [
    "similarity matrix reprecents each sentence how smiliar it is to other sentence\n",
    "\n",
    "1. 1's in diagnoal reprecent the sentence it self\n",
    "2. other number reprecent are similarity match \n",
    "\n",
    "example\n",
    "\n",
    "one line one\n",
    "\n",
    "1.000 this is equal to \"This is python script\"\n",
    "other 3 numbers define how other sentence match with parent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b03072",
   "metadata": {},
   "source": [
    "# Reranker\n",
    "\n",
    "use reranker to rank the results and use the top ranking one as it is best posible match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4940fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoderModel=CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L6-v2\")\n",
    "\n",
    "query=\"How are clouds today\"\n",
    "\n",
    "passages=[\n",
    "    \"This is python script\",\n",
    "    \"This is embeding transformer\",\n",
    "    \"This converts text to numbers\",\n",
    "    \"Clouds are white today outside\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7205a314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each sentence compare it with query and get score\n",
    "score=encoderModel.predict([(query, passage) for passage in passages])  \n",
    "print(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1496ff39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on the score result get the rank\n",
    "ranks = encoderModel.rank(query, passages, return_documents=True)\n",
    "\n",
    "print(\"Query:\", query)\n",
    "for rank in ranks:\n",
    "    print(f\"- #{rank['corpus_id']} ({rank['score']:.2f}): {rank['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4608bfc0",
   "metadata": {},
   "source": [
    "# Sparce Encoder\n",
    "\n",
    "Unlike dense embeddings, most values in a sparse vector are zero. It excels at finding exact matches, technical terms, or rare words (like \"iPhone 15 Pro Max\" or a specific serial number) that a dense embedding might generalize too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb69ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Load a pretrained SparseEncoder model\n",
    "model = SparseEncoder(\"naver/splade-cocondenser-ensembledistil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b6f322",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# The sentences to encode\n",
    "sentences=[\n",
    "    \"This is python script\",\n",
    "    \"This is embeding transformer\",\n",
    "    \"This converts text to numbers\",\n",
    "    \"Clouds are white today outside\"\n",
    "]\n",
    "\n",
    "# 2. Calculate sparse embeddings by calling model.encode()\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings.shape)\n",
    " \n",
    "\n",
    "# 3. Calculate the embedding similarities\n",
    "similarities = model.similarity(embeddings, embeddings)\n",
    "print(similarities)\n",
    " \n",
    "\n",
    "# 4. Check sparsity stats\n",
    "stats = SparseEncoder.sparsity(embeddings)\n",
    "\n",
    "print(f\"Sparsity: {stats['sparsity_ratio']:.2%}\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92adec3e",
   "metadata": {},
   "source": [
    "as seen in simialrity \n",
    "\n",
    "1. matrix daigonal value is the sentence itself\n",
    "2. other number reprecent how exact match it is with other sentence\n",
    "3. unlike embadings where other sentence if they are similar it will rank. this looks for most abosule match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf07b07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
